## 온프레미스에서 로드밸런서를 제공하는 **MetalLB**

- 베어메탈(bare metal, 운영 체제가 설치되지 않은 하드웨어)로 구성된 쿠버네티스에서도 로드밸런서를 사용할 수 있게 고안된 프로젝트
- 특별한 네트워크 설정이나 구성이 있는 것이 아니라 기존의 L2 네트워크(ARP/NDP)와 L3 네트워크(BGP)로 로드밸런서를 구현
- MetalLB 컨트롤러는 작동 방식(Protocol, 프로토콜)을 정의하고, EXTERNAL-IP를 부여해 관리
- MetalLB 스피커는 정해진 작동방식(L2/ARP, L3/BGP)에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고 수집해 각 파드의 경로를 제공. 이때 L2는 스피커 중에서 리더를 선출해 경로 제공을 총괄하게 함

### MetalLB 로드밸런서 서비스 구현

: 두 개의 MetalLB의 네트워크로 로드밸런서를 구현

![Untitled](https://user-images.githubusercontent.com/90545926/160279690-61536754-21c6-4973-8f40-0a54c4644440.png)

- 디플로이먼트를 이용해 2종류(lb-hname-pods, lb-ip-pods)의 파드를 생성

```bash
[root@m-k8s ~]# kubectl create deployment lb-hname-pods --image=sysnet4admin/echo-hname
deployment.apps/lb-hname-pods created

[root@m-k8s ~]# kubectl create deployment lb-ip-pods --image=sysnet4admin/echo-ip
deployment.apps/lb-ip-pods created

[root@m-k8s ~]# kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
lb-hname-pods-79b95c7c7b-ght6n   1/1     Running   0          2m
lb-ip-pods-6c6bb59b4-fxpbw       1/1     Running   0          108s
```

- scale 명령으로 파드를 3개로 늘려 노드당 1개씩 파드가 배포되게 함

```bash
[root@m-k8s ~]# kubectl scale deployment lb-hname-pods --replicas=3
deployment.apps/lb-hname-pods scaled

[root@m-k8s ~]# kubectl scale deployment lb-ip-pods --replicas=3
deployment.apps/lb-ip-pods scaled

[root@m-k8s ~]# kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
lb-hname-pods-79b95c7c7b-859lk   1/1     Running   0          25s
lb-hname-pods-79b95c7c7b-ght6n   1/1     Running   0          3m36s
lb-hname-pods-79b95c7c7b-whvjs   1/1     Running   0          25s
lb-ip-pods-6c6bb59b4-8trrv       1/1     Running   0          19s
lb-ip-pods-6c6bb59b4-fxpbw       1/1     Running   0          3m24s
lb-ip-pods-6c6bb59b4-pzn4n       1/1     Running   0          19s
```

- 사전에 정의된 오브젝트 스펙으로 MetalLB를 구성

  - MetalLB

  ```bash
  <https://raw.githubusercontent.com/metallb/metallb/v0.8.3/manifests/metallb.yaml>
  ```

  - MetalLB에 필요한 요소가 모두 설치되고 독립적인 네임스페이스도 함께 만들어짐

  ```bash
  [root@m-k8s ~]# kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.4/metallb.yaml
  namespace/metallb-system created
  podsecuritypolicy.policy/speaker created
  serviceaccount/controller created
  serviceaccount/speaker created
  clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
  clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
  role.rbac.authorization.k8s.io/config-watcher created
  clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
  clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
  rolebinding.rbac.authorization.k8s.io/config-watcher created
  daemonset.apps/speaker created
  deployment.apps/controller created
  ```

- 배포된 MetalLB의 파드가 5개(controller 1개, speaker 4개)인지 확인하고, IP와 상태 확인

```bash
[root@m-k8s ~]# kubectl get pods -n metallb-system -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
controller-5f98465b6b-mjclq   1/1     Running   0          5m40s   172.16.221.136   w1-k8s   <none>           <none>
speaker-6vf42                 1/1     Running   0          5m40s   192.168.1.103    w3-k8s   <none>           <none>
speaker-7pvft                 1/1     Running   0          5m40s   192.168.1.10     m-k8s    <none>           <none>
speaker-hrpkh                 1/1     Running   0          5m40s   192.168.1.102    w2-k8s   <none>           <none>
speaker-kj64t                 1/1     Running   0          5m40s   192.168.1.101    w1-k8s   <none>           <none>
```

- 인그레스와 마찬가지로 MetalLB도 설정을 적용

  - metallb-l2config.yaml

  ```bash
  apiVersion: v1
  kind: ConfigMap
  metadata:
    namespace: metallb-system #네임스페이스 이름
    name: config #컨피그맵 이름
  data:
    config: | #설정내용
      address-pools: # metalLB의 세부설정
      - name: nginx-ip-range
        protocol: layer2 # metallb에서 제공하는 로드밸런서의 동작 방식
        addresses: # metallb에서 제공하는 로드밸런스의 Ext 주소
        - 192.168.1.11-192.168.1.13
  ```

  - 오브젝트는 ConfigMap을 사용, ConfigMap이 생성됐는지 확인

  ```bash
  [root@m-k8s ~]# kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.4/metallb-l2config.yaml
  configmap/config created
  
  [root@m-k8s ~]# kubectl get configmap -n metallb-system
  NAME     DATA   AGE
  config   1      14s
  ```

  ```bash
  [root@m-k8s ~]# kubectl get configmap -n metallb-system -o yaml
  apiVersion: v1
  items:
  - apiVersion: v1
    data:
      config: |
        address-pools:
        - name: nginx-ip-range
          protocol: layer2
          addresses:
          - 192.168.1.11-192.168.1.13
    kind: ConfigMap
    metadata:
      annotations:
        kubectl.kubernetes.io/last-applied-configuration: |
          {"apiVersion":"v1","data":{"config":"address-pools:\\n- name: nginx-ip-range\\n  protocol: layer2\\n  addresses:\\n  - 192.168.1.11-192.168.1.13\\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"config","namespace":"metallb-system"}}
      creationTimestamp: "2022-03-26T12:07:48Z"
      managedFields:
      - apiVersion: v1
        fieldsType: FieldsV1
        fieldsV1:
          f:data:
            .: {}
            f:config: {}
          f:metadata:
            f:annotations:
              .: {}
              f:kubectl.kubernetes.io/last-applied-configuration: {}
        manager: kubectl
        operation: Update
        time: "2022-03-26T12:07:48Z"
      name: config
      namespace: metallb-system
      resourceVersion: "1065470"
      selfLink: /api/v1/namespaces/metallb-system/configmaps/config
      uid: 252e3066-f6a8-4f27-a338-9870bd3fcf89
  kind: List
  metadata:
    resourceVersion: ""
    selfLink: ""
  ```

- 모든 설정이 완료되어 각 디플로이먼트((lb-hname-pods, lb-ip-pods)를 로드밸런서 서비스로 노출

```bash
[root@m-k8s ~]# kubectl expose deployment lb-hname-pods --type=LoadBalancer --name=lb-hname-svc --port=80
service/lb-hname-svc exposed
[root@m-k8s ~]# kubectl expose deployment lb-ip-pods --type=LoadBalancer --name=lb-ip-svc --port=80
service/lb-ip-svc exposed
```

- 생성된 로드밸런서 서비스별로 CLUSTER-IP와 EXTERNAL-IP가 잘 적용됐는지 확인(특히 EXTERNAL-IP에 ConfigMap을 통해 부여한 IP를 확인)

```bash
[root@m-k8s ~]# kubectl get services
NAME           TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
kubernetes     ClusterIP      10.96.0.1        <none>         443/TCP        12d
lb-hname-svc   LoadBalancer   10.106.243.195   192.168.1.11   80:31881/TCP   93s
lb-ip-svc      LoadBalancer   10.99.249.208    192.168.1.12   80:30824/TCP   87s
```

- EXTERNAL-IP 작동 확인

![Untitled](https://user-images.githubusercontent.com/90545926/160279691-12c36730-7910-4099-ae4f-3a74d5968c01.png)

![Untitled](https://user-images.githubusercontent.com/90545926/160279692-23c819e0-32c5-40ed-94cd-76db89c17734.png)

- powershell에서 로드밸런서 기능 테스트

```bash
PS C:\\Users\\loverdock> $i=0; while($true)
>> {
>>     % { $i++; write-host -NoNewline "$i $_" }
>>     (Invoke-RestMethod "<http://192.168.1.11>")-replace '\\n', " "
>> }
1 lb-hname-pods-79b95c7c7b-whvjs
2 lb-hname-pods-79b95c7c7b-whvjs
3 lb-hname-pods-79b95c7c7b-whvjs
4 lb-hname-pods-79b95c7c7b-whvjs
```

![Untitled](https://user-images.githubusercontent.com/90545926/160279693-b114a300-a6be-4925-86be-1604b9dce653.png)

- 파드를 6개로 늘리고 테스트

```bash
[root@m-k8s ~]# kubectl scale deployment lb-hname-pods --replicas=6

[root@m-k8s ~]# kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
lb-hname-pods-79b95c7c7b-859lk   1/1     Running   0          30m
lb-hname-pods-79b95c7c7b-ght6n   1/1     Running   0          33m
lb-hname-pods-79b95c7c7b-n8nqt   1/1     Running   0          27s
lb-hname-pods-79b95c7c7b-s2sxr   1/1     Running   0          27s
lb-hname-pods-79b95c7c7b-vbhc9   1/1     Running   0          27s
lb-hname-pods-79b95c7c7b-whvjs   1/1     Running   0          30m
lb-ip-pods-6c6bb59b4-8trrv       1/1     Running   0          29m
lb-ip-pods-6c6bb59b4-fxpbw       1/1     Running   0          33m
lb-ip-pods-6c6bb59b4-pzn4n       1/1     Running   0          29m
```

![Untitled](https://user-images.githubusercontent.com/90545926/160279694-2f9b8580-dab7-487d-b0c5-0165cb0d42b9.png)

- 배포한 디플로이먼트와 서비스 삭제

```bash
[root@m-k8s ~]# kubectl delete deployment lb-hname-pods
deployment.apps "lb-hname-pods" deleted
[root@m-k8s ~]# kubectl delete deployment lb-ip-pods
deployment.apps "lb-ip-pods" deleted
[root@m-k8s ~]# kubectl delete service lb-hname-svc
service "lb-hname-svc" deleted
[root@m-k8s ~]# kubectl delete service lb-ip-svc
service "lb-ip-svc" deleted
```

### 부하에 따라 자동으로 파드 수를 조절하는 HPA

- HPA(Horizonal Pod Autoscaler): 사용자가 갑자기 들어 부하가 늘어나는 경우 쿠버네티스는 부하량에 따라 디플로이먼트 파드 수를 유동적으로 관리하는 기능을 제공
- 디플로이먼트 1개를 hpa-hname-pod라는 이름으로 생성

```bash
[root@m-k8s ~]# kubectl create deployment hpa-hname-pods --image=sysnet4admin/echo-hname
deployment.apps/hpa-hname-pods created
```

- MetalLB는 이미 구성되어 있으므로 expose를 실행해 hpa-hname-pods를 로드밸런서 서비스로 바로 설정

```bash
[root@m-k8s ~]# kubectl expose deployment hpa-hname-pods --type=LoadBalancer --name=hpa-hname-svc --port=80
service/hpa-hname-svc exposed

[root@m-k8s ~]# kubectl get services
NAME            TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE
hpa-hname-svc   LoadBalancer   10.102.52.121   192.168.1.11   80:30762/TCP   27s
kubernetes      ClusterIP      10.96.0.1       <none>         443/TCP        12d
```

- 계측값을 수집하고 전달해주는 메트릭 서버 설정(오브젝트 스펙파일로 설치할 수 있음)

  - HPA를 작동하려면 파드의 자원이 어느 정도 사용되는지 확인 필요. HPA가 자원을 요청할 때 메트릭 서버(Metrics-Server)를 통해 계측값을 전달 받는데 현재 메트릭 서버가 없어 에러가 남 → 메트릭 서버 설정 필요

  ```bash
  [root@m-k8s ~]# kubectl top pods
  Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)
  ```

  - 오브젝트 스펙 파일이 여러 개라서 git clone 이후에 디렉터리에 있는 파일들을 다시 실행하야 하는 번거롭고, 추가 설정이 필요해 오브젝트 스펙파일을 수정함 (기존 파일을 그대로 사용하면 오류남)

  ```bash
  #기존 파일(<https://github.com/kubernetes-sigs/metrics-server>)에서 수정된 부분 line96~line106
  containers:
        - name: metrics-server
          image: k8s.gcr.io/metrics-server-amd64:v0.3.6
          args:
          # Manually Add for lab env(Sysnet4admin/k8s)
          # skip tls internal usage purpose
          # TLS(Transport Layer Security)를 무시하게 함
            - --kubelet-insecure-tls
          # kubelet could use internalIP communication 
          # kubelet이 내부 주소를 우선 사용하게 함
            - --kubelet-preferred-address-types=InternalIP
            - --cert-dir=/tmp
            - --secure-port=4443
  ```

  - 메트릭 서버를 생성

  ```bash
  [root@m-k8s ~]# kubectl create -f ~/_Book_k8sInfra/ch3/3.3.5/metrics-server.yaml
  clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
  clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
  rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
  apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
  serviceaccount/metrics-server created
  deployment.apps/metrics-server created
  service/metrics-server created
  clusterrole.rbac.authorization.k8s.io/system:metrics-server created
  clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
  ```

  - 부하를 확인(현재는 부하가 없어 CPU와 MEMORY 값이 낮게 나옴)

  ```bash
  [root@m-k8s ~]# kubectl top pods
  NAME                              CPU(cores)   MEMORY(bytes)   
  hpa-hname-pods-75f874d48c-2xgd8   0m           1Mi
  ```

- edit 명령어를 실행해 배포된 디플로이먼트 내용을 직접 수정

  - 40번째 줄 resources : {} 부분에서 {}을 생략하고, 그 아래 다음과 같이 requests, limits 항목과 그 값을 추가해 파드마다 주어진 부하량을 결정하는 기준을 정함(여기서 사용한 단위 m은 milliunits의 약어로 1000m은 1개의 CPU가 됨, 10m은 CPU 0.01 사용을 기준으로 파드를 증설하게 설정. 또한 순간적으로 한쪽 파드로 부하가 몰릴 경우를 대비해 CPU 사용 제한을 0.05로 줌)

  ```bash
  spec:
        containers:
        - image: sysnet4admin/echo-hname
          imagePullPolicy: Always
          name: echo-hname
          resources: 
            requests:
              cpu: "10m"
            limits:
              cpu: "50m"
          terminationMessagePath: /dev/termination-log
  ```

  ```bash
  [root@m-k8s ~]# kubectl edit deployment hpa-hname-pods
  deployment.apps/hpa-hname-pods edited
  ```

  - 스펙이 변경돼 새로운 파드가 생성된 것을 확인할 수 있음

  ```bash
  [root@m-k8s ~]# kubectl top pods
  NAME                              CPU(cores)   MEMORY(bytes)   
  hpa-hname-pods-696b8fcc99-744rb   1m           1Mi
  ```

- hpa-hname-pods에 autoscale을 설정해서 특정 조건이 만족되는 경우에 자동으로 scale명령이 수행되도록 함(min은 최소 파드 수, max는 최대 파드의 수, cpu-percent는 CPU 사용량이 50%를 넘게 되면 autoscale함)

```bash
[root@m-k8s ~]# kubectl autoscale deployment hpa-hname-pods --min=1 --max=30 --cpu-percent=50
horizontalpodautoscaler.autoscaling/hpa-hname-pods autoscaled
```

- 부하테스트

  - 마스터 노드 창 두개를 띄워 각각의 명령어를 사용 (watch를 사용해 2초에 한번씩 상태를 확인)

  ```bash
  watch kubectl top pods
  ```

  ```bash
  watch kubectl get pods
  ```

  ![Untitled](https://user-images.githubusercontent.com/90545926/160279698-1230c003-5d6f-449e-ab7e-e68085eff8a3.png)

  - powershell에는 반복문 실행

  ```bash
  $i=0; while($true)
  {
      % { $i++; write-host -NoNewline "$i $_" }
      (Invoke-RestMethod "<http://192.168.1.11>")-replace '\\n', " "
  }
  ```

  - 부하량이 늘어남에 따라 파드가 새로 생성됨을 확인

  ![Untitled](https://user-images.githubusercontent.com/90545926/160279699-9d2f2d37-ef84-465c-b4e7-e6a54a6ba832.png)

  ![Untitled](https://user-images.githubusercontent.com/90545926/160279700-f8fde8aa-ce57-43ad-b81e-1dbed4ae196e.png)

  - powershell을 종료하고 일정시간 지나 더이상 부하가 없으면 autoscale의 최소 조건인 파드 1가 한개로 돌아가는 것을 확인(시간이 더 오래 걸림)

  ![Untitled](https://user-images.githubusercontent.com/90545926/160279702-a459fc8e-644f-4173-ae50-208422ff3fb8.png)

  ![Untitled](https://user-images.githubusercontent.com/90545926/160279704-c64b2516-bd28-4b3b-86e4-e9c395f3710a.png)

- 생성한 디플로이먼트, 서비스 메트릭 서버를 삭제

```bash
[root@m-k8s ~]# kubectl delete deployment hpa-hname-pods
deployment.apps "hpa-hname-pods" deleted

[root@m-k8s ~]# kubectl delete hpa hpa-hname-pods
horizontalpodautoscaler.autoscaling "hpa-hname-pods" deleted

[root@m-k8s ~]# kubectl delete service hpa-hname-svc
service "hpa-hname-svc" deleted

[root@m-k8s ~]# kubectl delete -f ~/_Book_k8sInfra/ch3/3.3.5/metrics-server.yaml
clusterrole.rbac.authorization.k8s.io "system:aggregated-metrics-reader" deleted
clusterrolebinding.rbac.authorization.k8s.io "metrics-server:system:auth-delegator" deleted
rolebinding.rbac.authorization.k8s.io "metrics-server-auth-reader" deleted
apiservice.apiregistration.k8s.io "v1beta1.metrics.k8s.io" deleted
serviceaccount "metrics-server" deleted
deployment.apps "metrics-server" deleted
service "metrics-server" deleted
clusterrole.rbac.authorization.k8s.io "system:metrics-server" deleted
clusterrolebinding.rbac.authorization.k8s.io "system:metrics-server" deleted
```